# Introduction

text books：Data Mining Concepts and Techniques ; Mining of Massive Datasets; Introduction to Data Mining

http://www.mmds.org/

3Vs: Volume(Scale), Variety(Complexity), Velocity(Speed)

Big data analytics(definition): Big data analysis is Advanced analytic technologies operate on big data sets 

why: extract value from data; stored managed and analyzed

![2024-01-02_14h47_53](C:\school_data_file\int402\2024-01-02_14h47_53.png)



Descriptive mining:  Data characterization, Association analysis, Cluster analysis, outlier analysis

# Lecture1 Distributed computing

Chunk servers 服务器块

File is split into contiguous chunks ; Typically each chunk is 16-64MB ;Each chunk replicated

Data kept in “chunks” spread across machines Each chunk replicated on different machines

**MapReduce**

Map -> Group by key -> Reduce

Map: Mapper applies Map function to a single element, output is a set of 01 or more key-value pairs

Group by key: Sort and shuffle

Reduce: Reduce function is applied to each key

<img src="C:\school_data_file\int402\2024-01-02_15h11_58.png" alt="2024-01-02_15h11_58" style="zoom:50%;" /><img src="C:\school_data_file\int402\2024-01-02_15h13_47.png" alt="2024-01-02_15h13_47" style="zoom:50%;" />

**Spark**

Key concept:  **Resilient Distributed Dataset (RDD)**

<img src="C:\school_data_file\int402\image-20240102154724398.png" alt="image-20240102154724398" style="zoom:67%;" />

# Lecture2 Frequent Itemset Mining Association Rules

Naïve Algorithm

From each basket of n items, generate its n(n-1)/2 pairs by two nested loops

Suppose 105 items, counts are 4-byte integers  Number of pairs of items: 105(105-1)/2 ≈ 5*109  Therefore, 2*1010 (20 gigabytes) of memory needed

Pass1 of PCY:In addition to item counts, maintain a hash table with asmany buckets as fit in memory Keep a countfor each bucket into which pairs of items are hashed For eachbucketjust keep the count, not the actual pairs that hashto the bucket!

Count all pairs {i, j}that meet the conditions for being a candidate pair: Ø Both i and j are frequent items Ø The pair {i, j} hashes to a bucket whose bit in the bit vector is 1 (i.e., a frequent bucket

The Market-Basket Model

先验算法Apriori Algorithm是关联规则学习算法之一。先验算法的设计目的是为了处理包含交易信息内容的数据库（例如,顾客购买的商品清单，或者网页常访清单。）

见草稿纸

https://zhuanlan.zhihu.com/p/66944900

https://bainingchao.github.io/2018/09/27/%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99Apriori%E7%AE%97%E6%B3%95/

https://github.com/BaiNingchao/MachineLearning-1

https://github.com/apachecn/ailearning/blob/master/docs/ml/11.md

https://www.cnblogs.com/pinard/p/6307064.html

# Lecture3 Find Similar Items：Locality Sensitive Hashing

Given a **query Q**, find nearest neighbors FAST

近似近邻问题 Approximate Near Neighbor

https://www.cnblogs.com/maybe2030/p/4953039.html

局部敏感哈希(Locality-Sensitive Hashing, LSH)

- **局部敏感哈希**（Locality-Sensitive Hashing, LSH）指的是一类特殊的 Hash 函数——它们能使相似的高维数据在经过**降维**后依然保持一定的相似性。
- 局部敏感哈希的基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻；而原本不相邻的两个数据，在低维空间中也将有很大的概率不相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，避免在高维数据空间中寻找，因为在高维空间中会很耗时。有这样性质的哈希映射称为是局部敏感的。
- LSH 本质上是一种利用 Hash 进行快速**相似度计算**的算法。
- 具体来说，相似的高维数据经过 LSH 处理后，会以较高的**概率**进入相同的“**桶(bucket)**”中，从而达到分类相似数据的目的。

https://ansvver.github.io/lsh_minhash.html

3 essential steps for similar docs

Shingling, min-hashing, localoty-sensitive hashing

![image-20240104141547266](C:\school_data_file\int402\image-20240104141547266.png)

![image-20240104141525736](C:\school_data_file\int402\image-20240104141525736.png)













# Lecture4 Clustering

常见：Cluster Outlier

**Hierarchical Clustering**

Key operation： Repeatedly combine two nearest clusters

Euclidean case: each cluster has a **centroid**质心= average of its (data)points

![image-20240103144020543](C:\school_data_file\int402\image-20240103144020543.png)

**K-Means Algorithms**

Assumes Euclidean space/distance；Start by picking k, the number of clusters；Initialize clusters by picking one point per cluster

**K-Means++：**

初始质心选取的基本思路就是，**初始的聚类中心之间的相互距离要尽可能的远**。

Pick a small sample of points S, cluster them by any algorithm, and use the centroids as a seed ¢In k-means++, sample size S= k times a factor that is logarithmic in the total number of points

<img src="C:\school_data_file\int402\image-20240103145004848.png" alt="image-20240103145004848" style="zoom:50%;" />

How to select k

尝试不同的k，观察随着k的增加到质心的平均距离的变化; 均值迅速下降，直到右k，然后变化不大

# Lecture 5 Dimensionality Reduction

SVD -- singular value decomposition 奇异值分解

CUR decomposition



https://blog.csdn.net/lomodays207/article/details/88687126

https://blog.csdn.net/xo3ylAF9kGs/article/details/105941914

https://bainingchao.github.io/2018/10/11/%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/

https://zhuanlan.zhihu.com/p/29846048

# Lecture 6 7 Recommender System

X = set of Customers
S = set of Items
Utility function u: X × S à R
	R = set of ratings
	R is a totally ordered set

Three approaches to recommender systems: 

**1) Content-based 2) Collaborative 3) Latent factor based**

**Content-based Recommendations**: 推荐items 或者相似的items to customer x

so create an item profile概要：各种特征如作者 标题 演员

![s](C:\school_data_file\int402\image-20240103155037577.png)

好处：不需要其他用户的数据；能够向有独特品味的用户推荐；能够推荐新产品和不受欢迎的产品；能够提供解释(Able to provide explanations)

Harnessing quality judgments of other users

# Lecture10 Large Tree

Decision Tree:

Given an input data vector x predict y
